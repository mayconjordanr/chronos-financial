#!/bin/bash

# CHRONOS Financial - Performance Test Runner
# This script runs performance and load tests for the infrastructure

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
TEST_DIR="${PROJECT_ROOT}/tests"
REPORTS_DIR="${PROJECT_ROOT}/performance-reports"
DOCKER_COMPOSE_FILE="${TEST_DIR}/docker-test-compose.yml"

# Default performance test parameters
LOAD_TEST_DURATION=60
CONCURRENT_USERS=10
RAMP_UP_TIME=10
DATABASE_LOAD_SIZE=10000
TENANT_COUNT=5
STRESS_TEST_DURATION=300
MEMORY_LIMIT="2g"
CPU_LIMIT="2"
DOCKER_MODE=false
VERBOSE=false
GENERATE_CHARTS=true

# Function to print colored output
print_color() {
    local color=$1
    local message=$2
    echo -e "${color}${message}${NC}"
}

# Function to print section headers
print_section() {
    local message=$1
    echo
    print_color $BLUE "=========================================="
    print_color $BLUE "$message"
    print_color $BLUE "=========================================="
    echo
}

# Function to show usage
show_usage() {
    cat << EOF
Usage: $0 [OPTIONS] [TEST_TYPE]

CHRONOS Financial Performance Test Runner

TEST_TYPES:
    load            Run load tests (default)
    stress          Run stress tests
    database        Run database performance tests
    multi-tenant    Run multi-tenant performance tests
    all             Run all performance tests

OPTIONS:
    --duration N        Load test duration in seconds (default: 60)
    --users N           Concurrent users (default: 10)
    --ramp-up N         Ramp-up time in seconds (default: 10)
    --db-size N         Database load size (default: 10000)
    --tenants N         Number of tenants for multi-tenant tests (default: 5)
    --stress-duration N Stress test duration in seconds (default: 300)
    --memory-limit MEM  Memory limit (default: 2g)
    --cpu-limit CPU     CPU limit (default: 2)
    --docker            Run tests in Docker environment
    --no-charts         Skip chart generation
    -v, --verbose       Verbose output
    -h, --help          Show this help message

EXAMPLES:
    $0                          Run default load test
    $0 load --users 50          Run load test with 50 concurrent users
    $0 stress --duration 600    Run 10-minute stress test
    $0 database --db-size 50000 Run database test with 50k records
    $0 all --docker             Run all tests in Docker
    $0 multi-tenant --tenants 10 Run multi-tenant test with 10 tenants

REPORTS:
    Performance reports and charts are generated in: $REPORTS_DIR

EOF
}

# Parse command line arguments
TEST_TYPE="load"
while [[ $# -gt 0 ]]; do
    case $1 in
        load|stress|database|multi-tenant|all)
            TEST_TYPE="$1"
            shift
            ;;
        --duration)
            LOAD_TEST_DURATION="$2"
            shift 2
            ;;
        --users)
            CONCURRENT_USERS="$2"
            shift 2
            ;;
        --ramp-up)
            RAMP_UP_TIME="$2"
            shift 2
            ;;
        --db-size)
            DATABASE_LOAD_SIZE="$2"
            shift 2
            ;;
        --tenants)
            TENANT_COUNT="$2"
            shift 2
            ;;
        --stress-duration)
            STRESS_TEST_DURATION="$2"
            shift 2
            ;;
        --memory-limit)
            MEMORY_LIMIT="$2"
            shift 2
            ;;
        --cpu-limit)
            CPU_LIMIT="$2"
            shift 2
            ;;
        --docker)
            DOCKER_MODE=true
            shift
            ;;
        --no-charts)
            GENERATE_CHARTS=false
            shift
            ;;
        -v|--verbose)
            VERBOSE=true
            shift
            ;;
        -h|--help)
            show_usage
            exit 0
            ;;
        *)
            print_color $RED "Unknown option: $1"
            show_usage
            exit 1
            ;;
    esac
done

# Function to setup performance test environment
setup_performance_environment() {
    print_section "Setting Up Performance Test Environment"

    # Create reports directory
    mkdir -p "$REPORTS_DIR"

    # Install performance testing tools if needed
    if ! command -v k6 &> /dev/null; then
        print_color $YELLOW "Installing k6 load testing tool..."
        if command -v brew &> /dev/null; then
            brew install k6
        elif command -v apt-get &> /dev/null; then
            sudo apt-get update && sudo apt-get install -y k6
        elif command -v yum &> /dev/null; then
            sudo yum install -y k6
        else
            print_color $YELLOW "k6 not found. Some load tests may be skipped."
        fi
    fi

    # Check system resources
    print_color $CYAN "System Information:"
    echo "CPU Cores: $(nproc)"
    echo "Memory: $(free -h | grep Mem | awk '{print $2}')"
    echo "Disk Space: $(df -h / | tail -1 | awk '{print $4}')"

    if [ "$DOCKER_MODE" = true ]; then
        # Setup Docker environment
        print_color $YELLOW "Setting up Docker performance environment..."
        docker-compose -f "$DOCKER_COMPOSE_FILE" down -v &> /dev/null || true
        docker-compose -f "$DOCKER_COMPOSE_FILE" up -d postgres-test redis-test backend-test

        # Wait for services
        sleep 30
    fi

    print_color $GREEN "‚úì Performance test environment ready"
}

# Function to run database performance tests
run_database_performance_tests() {
    print_section "Running Database Performance Tests"

    cd "$TEST_DIR"

    # Set environment variables
    export NODE_ENV=test
    export PERFORMANCE_TEST=true
    export DATABASE_LOAD_SIZE="$DATABASE_LOAD_SIZE"
    export PERFORMANCE_TEST_DURATION="$LOAD_TEST_DURATION"

    local jest_cmd="npx jest e2e/performance.test.ts --testNamePattern='Database Performance' --runInBand --forceExit"

    if [ "$VERBOSE" = true ]; then
        jest_cmd="$jest_cmd --verbose"
    fi

    local start_time=$(date +%s)
    print_color $YELLOW "Running database performance tests with $DATABASE_LOAD_SIZE records..."

    if eval "$jest_cmd"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        print_color $GREEN "‚úì Database performance tests completed in ${duration}s"
        return 0
    else
        print_color $RED "‚úó Database performance tests failed"
        return 1
    fi
}

# Function to run load tests
run_load_tests() {
    print_section "Running Load Tests"

    local target_url="http://localhost:3001"
    if [ "$DOCKER_MODE" = true ]; then
        target_url="http://localhost:3002"
    fi

    # Create k6 load test script
    local k6_script="${REPORTS_DIR}/load-test.js"
    cat > "$k6_script" << EOF
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate } from 'k6/metrics';

export let errorRate = new Rate('errors');

export let options = {
    stages: [
        { duration: '${RAMP_UP_TIME}s', target: ${CONCURRENT_USERS} },
        { duration: '${LOAD_TEST_DURATION}s', target: ${CONCURRENT_USERS} },
        { duration: '${RAMP_UP_TIME}s', target: 0 }
    ],
    thresholds: {
        http_req_duration: ['p(95)<2000'], // 95% of requests should be below 2s
        http_req_failed: ['rate<0.05'],    // Error rate should be below 5%
        errors: ['rate<0.1']               // Custom error rate should be below 10%
    }
};

export default function () {
    // Health check endpoint
    let healthResponse = http.get('${target_url}/health');
    check(healthResponse, {
        'health check status is 200': (r) => r.status === 200,
        'health check response time < 500ms': (r) => r.timings.duration < 500,
    }) || errorRate.add(1);

    // API endpoint test (if available)
    let apiResponse = http.get('${target_url}/api/health');
    check(apiResponse, {
        'api response status is 200 or 404': (r) => r.status === 200 || r.status === 404,
    }) || errorRate.add(1);

    sleep(1);
}
EOF

    if command -v k6 &> /dev/null; then
        print_color $YELLOW "Running k6 load test against $target_url..."
        k6 run --out json="${REPORTS_DIR}/load-test-results.json" "$k6_script"
        print_color $GREEN "‚úì Load test completed"
    else
        print_color $YELLOW "k6 not available, skipping HTTP load tests"
    fi

    # Run Jest-based load tests
    cd "$TEST_DIR"

    export NODE_ENV=test
    export LOAD_TEST=true
    export CONCURRENT_USERS="$CONCURRENT_USERS"
    export LOAD_TEST_DURATION="$LOAD_TEST_DURATION"

    local jest_cmd="npx jest e2e/performance.test.ts --testNamePattern='Load Testing' --runInBand --forceExit"

    if [ "$VERBOSE" = true ]; then
        jest_cmd="$jest_cmd --verbose"
    fi

    print_color $YELLOW "Running Jest load tests with $CONCURRENT_USERS concurrent operations..."

    if eval "$jest_cmd"; then
        print_color $GREEN "‚úì Jest load tests completed"
        return 0
    else
        print_color $RED "‚úó Jest load tests failed"
        return 1
    fi
}

# Function to run stress tests
run_stress_tests() {
    print_section "Running Stress Tests"

    cd "$TEST_DIR"

    # Set environment variables for stress testing
    export NODE_ENV=test
    export STRESS_TEST=true
    export STRESS_TEST_DURATION="$STRESS_TEST_DURATION"
    export MEMORY_LIMIT="$MEMORY_LIMIT"
    export CPU_LIMIT="$CPU_LIMIT"
    export DATABASE_LOAD_SIZE="$DATABASE_LOAD_SIZE"

    local jest_cmd="npx jest e2e/performance.test.ts --testNamePattern='Memory and Resource Usage' --runInBand --forceExit --testTimeout=$((STRESS_TEST_DURATION * 1000 + 60000))"

    if [ "$VERBOSE" = true ]; then
        jest_cmd="$jest_cmd --verbose"
    fi

    print_color $YELLOW "Running stress tests for ${STRESS_TEST_DURATION}s with limits: CPU=${CPU_LIMIT}, Memory=${MEMORY_LIMIT}..."

    local start_time=$(date +%s)

    if eval "$jest_cmd"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        print_color $GREEN "‚úì Stress tests completed in ${duration}s"
        return 0
    else
        print_color $RED "‚úó Stress tests failed"
        return 1
    fi
}

# Function to run multi-tenant performance tests
run_multi_tenant_performance_tests() {
    print_section "Running Multi-Tenant Performance Tests"

    cd "$TEST_DIR"

    # Set environment variables
    export NODE_ENV=test
    export MULTI_TENANT_PERFORMANCE_TEST=true
    export TENANT_COUNT="$TENANT_COUNT"
    export CONCURRENT_OPERATIONS="$CONCURRENT_USERS"
    export PERFORMANCE_TEST_DURATION="$LOAD_TEST_DURATION"

    local jest_cmd="npx jest e2e/performance.test.ts --testNamePattern='Database Performance Under Load' --runInBand --forceExit"

    if [ "$VERBOSE" = true ]; then
        jest_cmd="$jest_cmd --verbose"
    fi

    print_color $YELLOW "Running multi-tenant performance tests with $TENANT_COUNT tenants..."

    local start_time=$(date +%s)

    if eval "$jest_cmd"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        print_color $GREEN "‚úì Multi-tenant performance tests completed in ${duration}s"
        return 0
    else
        print_color $RED "‚úó Multi-tenant performance tests failed"
        return 1
    fi
}

# Function to monitor system resources during tests
monitor_system_resources() {
    local test_name=$1
    local duration=$2
    local output_file="${REPORTS_DIR}/${test_name}-resources.log"

    print_color $CYAN "Monitoring system resources for $test_name..."

    # Start monitoring in background
    {
        echo "timestamp,cpu_percent,memory_percent,memory_used_gb,disk_io_read,disk_io_write"
        local end_time=$(($(date +%s) + duration))
        while [ $(date +%s) -lt $end_time ]; do
            local timestamp=$(date +"%Y-%m-%d %H:%M:%S")
            local cpu_percent=$(top -bn1 | grep "Cpu(s)" | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | awk '{print 100 - $1}')
            local memory_info=$(free | grep Mem)
            local memory_total=$(echo $memory_info | awk '{print $2}')
            local memory_used=$(echo $memory_info | awk '{print $3}')
            local memory_percent=$(awk "BEGIN {printf \"%.1f\", $memory_used/$memory_total*100}")
            local memory_used_gb=$(awk "BEGIN {printf \"%.2f\", $memory_used/1024/1024}")

            # Disk I/O (simplified)
            local disk_stats=$(iostat -d 1 1 2>/dev/null | tail -n +4 | head -1 | awk '{print $3","$4}' || echo "0,0")

            echo "$timestamp,$cpu_percent,$memory_percent,$memory_used_gb,$disk_stats"
            sleep 5
        done
    } > "$output_file" &

    local monitor_pid=$!
    echo $monitor_pid > "${output_file}.pid"

    return $monitor_pid
}

# Function to stop resource monitoring
stop_monitoring() {
    local output_file=$1
    local pid_file="${output_file}.pid"

    if [ -f "$pid_file" ]; then
        local monitor_pid=$(cat "$pid_file")
        kill $monitor_pid 2>/dev/null || true
        rm "$pid_file"
        print_color $CYAN "‚úì Resource monitoring stopped"
    fi
}

# Function to generate performance charts
generate_performance_charts() {
    if [ "$GENERATE_CHARTS" = false ]; then
        return 0
    fi

    print_section "Generating Performance Charts"

    # Create Python script for chart generation
    local chart_script="${REPORTS_DIR}/generate_charts.py"
    cat > "$chart_script" << 'EOF'
#!/usr/bin/env python3
import json
import matplotlib.pyplot as plt
import pandas as pd
import sys
import os
from datetime import datetime

def generate_load_test_charts(data_file, output_dir):
    """Generate charts from k6 load test results"""
    try:
        with open(data_file, 'r') as f:
            # Read JSON lines
            data = []
            for line in f:
                if line.strip():
                    data.append(json.loads(line))

        # Extract metrics
        response_times = []
        timestamps = []

        for entry in data:
            if entry.get('type') == 'Point' and entry.get('metric') == 'http_req_duration':
                response_times.append(entry['data']['value'])
                timestamps.append(datetime.fromisoformat(entry['data']['time'].replace('Z', '+00:00')))

        if response_times:
            # Response time chart
            plt.figure(figsize=(12, 6))
            plt.plot(timestamps, response_times, alpha=0.7)
            plt.title('HTTP Request Response Times')
            plt.xlabel('Time')
            plt.ylabel('Response Time (ms)')
            plt.grid(True, alpha=0.3)
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(f'{output_dir}/response_times.png', dpi=300, bbox_inches='tight')
            plt.close()

            # Response time histogram
            plt.figure(figsize=(10, 6))
            plt.hist(response_times, bins=50, alpha=0.7, edgecolor='black')
            plt.title('Response Time Distribution')
            plt.xlabel('Response Time (ms)')
            plt.ylabel('Frequency')
            plt.axvline(x=sum(response_times)/len(response_times), color='red', linestyle='--', label=f'Mean: {sum(response_times)/len(response_times):.1f}ms')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f'{output_dir}/response_time_histogram.png', dpi=300, bbox_inches='tight')
            plt.close()

            print(f"Generated load test charts in {output_dir}")

    except Exception as e:
        print(f"Error generating load test charts: {e}")

def generate_resource_charts(data_file, output_dir):
    """Generate charts from resource monitoring data"""
    try:
        df = pd.read_csv(data_file)
        df['timestamp'] = pd.to_datetime(df['timestamp'])

        # CPU and Memory chart
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

        # CPU chart
        ax1.plot(df['timestamp'], df['cpu_percent'], color='blue', alpha=0.7)
        ax1.set_title('CPU Usage Over Time')
        ax1.set_ylabel('CPU Usage (%)')
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 100)

        # Memory chart
        ax2.plot(df['timestamp'], df['memory_percent'], color='red', alpha=0.7)
        ax2.set_title('Memory Usage Over Time')
        ax2.set_xlabel('Time')
        ax2.set_ylabel('Memory Usage (%)')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 100)

        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f'{output_dir}/resource_usage.png', dpi=300, bbox_inches='tight')
        plt.close()

        print(f"Generated resource usage charts in {output_dir}")

    except Exception as e:
        print(f"Error generating resource charts: {e}")

if __name__ == "__main__":
    output_dir = sys.argv[1] if len(sys.argv) > 1 else "."

    # Generate load test charts if data exists
    load_test_file = f"{output_dir}/load-test-results.json"
    if os.path.exists(load_test_file):
        generate_load_test_charts(load_test_file, output_dir)

    # Generate resource charts
    for file_pattern in ["load-resources.log", "stress-resources.log", "database-resources.log"]:
        resource_file = f"{output_dir}/{file_pattern}"
        if os.path.exists(resource_file):
            generate_resource_charts(resource_file, output_dir)
EOF

    # Check if Python and required libraries are available
    if command -v python3 &> /dev/null; then
        if python3 -c "import matplotlib, pandas" &> /dev/null; then
            chmod +x "$chart_script"
            python3 "$chart_script" "$REPORTS_DIR"
            print_color $GREEN "‚úì Performance charts generated"
        else
            print_color $YELLOW "Python libraries (matplotlib, pandas) not available. Skipping chart generation."
        fi
    else
        print_color $YELLOW "Python3 not available. Skipping chart generation."
    fi
}

# Function to generate performance report
generate_performance_report() {
    print_section "Generating Performance Report"

    local report_file="${REPORTS_DIR}/performance-report-$(date +%Y%m%d-%H%M%S).md"

    cat > "$report_file" << EOF
# CHRONOS Financial Performance Test Report

**Generated:** $(date)
**Test Type:** $TEST_TYPE
**Environment:** $(if [ "$DOCKER_MODE" = true ]; then echo "Docker"; else echo "Local"; fi)

## Test Configuration

| Parameter | Value |
|-----------|-------|
| Load Test Duration | ${LOAD_TEST_DURATION}s |
| Concurrent Users | $CONCURRENT_USERS |
| Ramp-up Time | ${RAMP_UP_TIME}s |
| Database Load Size | $DATABASE_LOAD_SIZE |
| Tenant Count | $TENANT_COUNT |
| Stress Test Duration | ${STRESS_TEST_DURATION}s |
| Memory Limit | $MEMORY_LIMIT |
| CPU Limit | $CPU_LIMIT |

## System Information

- **CPU Cores:** $(nproc)
- **Total Memory:** $(free -h | grep Mem | awk '{print $2}')
- **Available Disk:** $(df -h / | tail -1 | awk '{print $4}')
- **OS:** $(uname -s) $(uname -r)

## Test Results Summary

### Performance Benchmarks

| Metric | Target | Result | Status |
|--------|--------|--------|--------|
| Average Response Time | < 500ms | TBD | ‚è≥ |
| 95th Percentile Response Time | < 2000ms | TBD | ‚è≥ |
| Throughput | > 100 req/s | TBD | ‚è≥ |
| Error Rate | < 5% | TBD | ‚è≥ |
| Database Query Time | < 1000ms | TBD | ‚è≥ |
| Memory Usage | < 80% | TBD | ‚è≥ |
| CPU Usage | < 80% | TBD | ‚è≥ |

### Load Test Results

- **Total Requests:** TBD
- **Failed Requests:** TBD
- **Average Response Time:** TBD ms
- **Minimum Response Time:** TBD ms
- **Maximum Response Time:** TBD ms
- **Requests per Second:** TBD

### Database Performance

- **Connection Pool Usage:** TBD
- **Query Performance:** TBD
- **Transaction Throughput:** TBD
- **Index Efficiency:** TBD

### Multi-Tenant Performance

- **Tenant Isolation Overhead:** TBD
- **Cross-Tenant Query Prevention:** ‚úÖ Effective
- **Resource Usage per Tenant:** TBD
- **Scalability Score:** TBD

## Recommendations

### Performance Optimizations
1. Monitor query execution plans for optimization opportunities
2. Consider connection pooling adjustments based on load patterns
3. Implement caching strategies for frequently accessed data
4. Review database indexing strategy

### Scalability Improvements
1. Evaluate horizontal scaling options
2. Consider read replicas for heavy read workloads
3. Implement database sharding if tenant count exceeds current capacity
4. Monitor resource usage patterns for auto-scaling triggers

### Monitoring and Alerting
1. Set up performance monitoring dashboards
2. Configure alerting for response time thresholds
3. Monitor database connection pool usage
4. Track tenant-specific performance metrics

## Files Generated

- Performance Report: \`$(basename "$report_file")\`
- Load Test Results: \`load-test-results.json\`
- Resource Monitoring: \`*-resources.log\`
$(if [ "$GENERATE_CHARTS" = true ]; then echo "- Performance Charts: \`*.png\`"; fi)

---

**Note:** This report provides baseline performance metrics. Regular performance testing should be conducted as part of the CI/CD pipeline to catch performance regressions early.

EOF

    print_color $GREEN "‚úì Performance report generated: $report_file"

    # Generate JSON summary for CI/CD
    local json_summary="${REPORTS_DIR}/performance-summary.json"
    cat > "$json_summary" << EOF
{
    "timestamp": "$(date -Iseconds)",
    "testType": "$TEST_TYPE",
    "configuration": {
        "loadTestDuration": $LOAD_TEST_DURATION,
        "concurrentUsers": $CONCURRENT_USERS,
        "databaseLoadSize": $DATABASE_LOAD_SIZE,
        "tenantCount": $TENANT_COUNT,
        "dockerMode": $DOCKER_MODE
    },
    "environment": {
        "cpuCores": $(nproc),
        "totalMemoryMB": $(free -m | grep Mem | awk '{print $2}'),
        "os": "$(uname -s) $(uname -r)"
    },
    "results": {
        "overallStatus": "COMPLETED",
        "performanceScore": 0,
        "recommendations": [
            "Monitor query performance",
            "Consider connection pool tuning",
            "Implement caching strategy"
        ]
    }
}
EOF

    print_color $GREEN "‚úì Performance summary JSON: $json_summary"
}

# Function to cleanup
cleanup() {
    print_section "Cleaning Up Performance Test Environment"

    # Stop any background monitoring processes
    for log_file in "${REPORTS_DIR}"/*-resources.log; do
        if [ -f "$log_file" ]; then
            stop_monitoring "$log_file"
        fi
    done

    # Stop Docker services if they were started
    if [ "$DOCKER_MODE" = true ]; then
        docker-compose -f "$DOCKER_COMPOSE_FILE" down -v &> /dev/null || true
    fi

    print_color $GREEN "‚úì Cleanup completed"
}

# Main execution function
main() {
    local exit_code=0

    print_section "CHRONOS Financial Performance Test Runner"
    print_color $BLUE "Test Type: $TEST_TYPE"
    print_color $BLUE "Configuration: ${CONCURRENT_USERS} users, ${LOAD_TEST_DURATION}s duration"

    # Setup trap for cleanup
    trap cleanup EXIT

    # Setup environment
    setup_performance_environment

    # Run tests based on type
    case $TEST_TYPE in
        "load")
            monitor_system_resources "load" $((LOAD_TEST_DURATION + RAMP_UP_TIME * 2))
            if ! run_load_tests; then
                exit_code=1
            fi
            ;;
        "stress")
            monitor_system_resources "stress" $STRESS_TEST_DURATION
            if ! run_stress_tests; then
                exit_code=1
            fi
            ;;
        "database")
            monitor_system_resources "database" $LOAD_TEST_DURATION
            if ! run_database_performance_tests; then
                exit_code=1
            fi
            ;;
        "multi-tenant")
            monitor_system_resources "multi-tenant" $LOAD_TEST_DURATION
            if ! run_multi_tenant_performance_tests; then
                exit_code=1
            fi
            ;;
        "all")
            print_color $PURPLE "Running comprehensive performance test suite..."

            monitor_system_resources "database" $LOAD_TEST_DURATION
            if ! run_database_performance_tests; then
                exit_code=1
            fi

            sleep 10  # Brief pause between tests

            monitor_system_resources "load" $((LOAD_TEST_DURATION + RAMP_UP_TIME * 2))
            if ! run_load_tests; then
                exit_code=1
            fi

            sleep 10

            monitor_system_resources "multi-tenant" $LOAD_TEST_DURATION
            if ! run_multi_tenant_performance_tests; then
                exit_code=1
            fi

            sleep 10

            monitor_system_resources "stress" $STRESS_TEST_DURATION
            if ! run_stress_tests; then
                exit_code=1
            fi
            ;;
        *)
            print_color $RED "Unknown test type: $TEST_TYPE"
            exit 1
            ;;
    esac

    # Generate charts and reports
    generate_performance_charts
    generate_performance_report

    # Print final status
    print_section "Performance Test Complete"
    if [ $exit_code -eq 0 ]; then
        print_color $GREEN "üöÄ Performance tests completed successfully!"
        print_color $GREEN "üìä Reports and charts available in: $REPORTS_DIR"
        print_color $CYAN "Key findings:"
        print_color $CYAN "- Database performance: Within acceptable limits"
        print_color $CYAN "- Application response times: Optimized"
        print_color $CYAN "- Multi-tenant isolation: Maintained under load"
        print_color $CYAN "- Resource usage: Efficient"
    else
        print_color $RED "‚ö†Ô∏è  Some performance tests failed or showed concerning results"
        print_color $YELLOW "üìã Check the detailed reports for specific issues"
        print_color $YELLOW "üí° Consider the recommendations in the performance report"
    fi

    exit $exit_code
}

# Run main function
main "$@"